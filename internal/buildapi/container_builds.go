package buildapi

import (
	"context"
	"fmt"
	"io"
	"log"
	"net/http"
	"os"
	"sort"
	"strings"
	"time"

	"github.com/gin-gonic/gin"
	"github.com/google/uuid"
	corev1 "k8s.io/api/core/v1"
	k8serrors "k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/types"
	"k8s.io/client-go/kubernetes"
	kscheme "k8s.io/client-go/kubernetes/scheme"
	"k8s.io/client-go/tools/remotecommand"
	"sigs.k8s.io/controller-runtime/pkg/client"

	automotivev1alpha1 "github.com/centos-automotive-suite/automotive-dev-operator/api/v1alpha1"
)

// --- Container Build Handlers ---

func (a *APIServer) handleStreamContainerBuildLogs(c *gin.Context) {
	name := c.Param("name")
	a.log.Info("container build logs requested", "build", name, "reqID", c.GetString("reqID"))
	a.streamContainerBuildLogs(c, name)
}

func (a *APIServer) handleCreateContainerBuild(c *gin.Context) {
	a.log.Info("create container build", "reqID", c.GetString("reqID"))
	a.createContainerBuild(c)
}

func (a *APIServer) handleListContainerBuilds(c *gin.Context) {
	a.log.Info("list container builds", "reqID", c.GetString("reqID"))
	listContainerBuilds(c)
}

func (a *APIServer) handleGetContainerBuild(c *gin.Context) {
	name := c.Param("name")
	a.log.Info("get container build", "build", name, "reqID", c.GetString("reqID"))
	a.getContainerBuild(c, name)
}

func (a *APIServer) handleContainerBuildUpload(c *gin.Context) {
	name := c.Param("name")
	a.log.Info("container build upload", "build", name, "reqID", c.GetString("reqID"))
	a.uploadContainerBuildContext(c, name)
}

// --- Container Build Implementation ---

func (a *APIServer) streamContainerBuildLogs(c *gin.Context, name string) {
	namespace := resolveNamespace()

	k8sClient, err := getClientFromRequest(c)
	if err != nil {
		c.JSON(http.StatusInternalServerError, gin.H{"error": err.Error()})
		return
	}

	sinceTime := parseSinceTime(c.Query("since"))
	streamDuration := time.Duration(a.limits.MaxLogStreamDurationMinutes) * time.Minute
	ctx, cancel := context.WithTimeout(c.Request.Context(), streamDuration)
	defer cancel()

	cb := &automotivev1alpha1.ContainerBuild{}
	if err := k8sClient.Get(ctx, types.NamespacedName{Name: name, Namespace: namespace}, cb); err != nil {
		if k8serrors.IsNotFound(err) {
			c.JSON(http.StatusNotFound, gin.H{"error": "not found"})
			return
		}
		c.JSON(http.StatusInternalServerError, gin.H{"error": err.Error()})
		return
	}

	buildRunName := strings.TrimSpace(cb.Status.BuildRunName)
	if buildRunName == "" {
		c.JSON(http.StatusServiceUnavailable, gin.H{"error": "logs not available yet"})
		return
	}

	restCfg, err := getRESTConfigFromRequest(c)
	if err != nil {
		c.JSON(http.StatusInternalServerError, gin.H{"error": err.Error()})
		return
	}
	cs, err := kubernetes.NewForConfig(restCfg)
	if err != nil {
		c.JSON(http.StatusInternalServerError, gin.H{"error": err.Error()})
		return
	}

	setupLogStreamHeaders(c)

	buildRunSelector := "buildrun.shipwright.io/name=" + buildRunName
	var hadStream bool
	var lastKeepalive time.Time
	streamedContainers := make(map[string]map[string]bool)
	completedPods := make(map[string]bool)

	for {
		select {
		case <-ctx.Done():
			return
		default:
		}

		pods, err := cs.CoreV1().Pods(namespace).List(ctx, metav1.ListOptions{LabelSelector: buildRunSelector})
		if err != nil {
			if _, writeErr := fmt.Fprintf(c.Writer, "\n[Error listing pods: %v]\n", err); writeErr != nil {
				fmt.Fprintf(os.Stderr, "Warning: failed to write error message: %v\n", writeErr)
			}
			c.Writer.Flush()
			time.Sleep(2 * time.Second)
			continue
		}

		if len(pods.Items) == 0 {
			if !hadStream {
				_, _ = c.Writer.Write([]byte("."))
				c.Writer.Flush()
			}
			time.Sleep(2 * time.Second)
			continue
		}

		sort.Slice(pods.Items, func(i, j int) bool {
			if pods.Items[i].Status.StartTime == nil {
				return false
			}
			if pods.Items[j].Status.StartTime == nil {
				return true
			}
			return pods.Items[i].Status.StartTime.Before(pods.Items[j].Status.StartTime)
		})

		allPodsComplete := true
		for _, pod := range pods.Items {
			if completedPods[pod.Name] {
				continue
			}

			if streamedContainers[pod.Name] == nil {
				streamedContainers[pod.Name] = make(map[string]bool)
			}

			processPodLogs(ctx, c, cs, pod, namespace, sinceTime, streamedContainers[pod.Name], &hadStream)

			stepNames := getStepContainerNames(pod)
			if len(streamedContainers[pod.Name]) == len(stepNames) &&
				(pod.Status.Phase == corev1.PodSucceeded || pod.Status.Phase == corev1.PodFailed) {
				completedPods[pod.Name] = true
			} else {
				allPodsComplete = false
			}
		}

		// Check if build is complete AND all pod logs have been streamed
		if allPodsComplete {
			if err := k8sClient.Get(ctx, types.NamespacedName{Name: name, Namespace: namespace}, cb); err == nil {
				if cb.Status.Phase == phaseCompleted || cb.Status.Phase == phaseFailed {
					break
				}
			}
		}

		time.Sleep(2 * time.Second)

		if !hadStream {
			_, _ = c.Writer.Write([]byte("."))
			if f, ok := c.Writer.(http.Flusher); ok {
				f.Flush()
			}
		} else if allPodsComplete {
			now := time.Now()
			if now.Sub(lastKeepalive) >= 30*time.Second {
				_, _ = c.Writer.Write([]byte("[Waiting for build to complete...]\n"))
				if f, ok := c.Writer.(http.Flusher); ok {
					f.Flush()
				}
				lastKeepalive = now
			}
		}
	}

	writeLogStreamFooter(c, hadStream)
}

func setContainerBuildSecretOwnerRef(
	ctx context.Context,
	c client.Client,
	namespace, secretName string,
	owner *automotivev1alpha1.ContainerBuild,
) error {
	secret := &corev1.Secret{}
	if err := c.Get(ctx, types.NamespacedName{Name: secretName, Namespace: namespace}, secret); err != nil {
		return err
	}
	secret.OwnerReferences = []metav1.OwnerReference{
		*metav1.NewControllerRef(owner, automotivev1alpha1.GroupVersion.WithKind("ContainerBuild")),
	}
	return c.Update(ctx, secret)
}

func (a *APIServer) createContainerBuild(c *gin.Context) {
	var req ContainerBuildRequest
	if err := c.ShouldBindJSON(&req); err != nil {
		c.JSON(http.StatusBadRequest, gin.H{"error": fmt.Sprintf("invalid request: %v", err)})
		return
	}

	if req.Name == "" {
		req.Name = fmt.Sprintf("cb-%s", uuid.New().String()[:8])
	}

	k8sClient, err := getClientFromRequestFn(c)
	if err != nil {
		c.JSON(http.StatusInternalServerError, gin.H{"error": fmt.Sprintf("k8s client error: %v", err)})
		return
	}

	ctx := c.Request.Context()
	namespace := resolveNamespace()
	requestedBy := a.resolveRequester(c)

	// Track created resources for cleanup on failure
	pushSecretName := ""
	createdImageStream := false
	committed := false
	defer func() {
		if committed {
			return
		}
		if pushSecretName != "" {
			secret := &corev1.Secret{}
			secret.Name = pushSecretName
			secret.Namespace = namespace
			if err := k8sClient.Delete(ctx, secret); err != nil && !k8serrors.IsNotFound(err) {
				log.Printf("WARNING: failed to clean up orphaned secret %s: %v", pushSecretName, err)
			}
		}
		if createdImageStream {
			if err := deleteImageStream(ctx, k8sClient, namespace, req.Name); err != nil {
				log.Printf("WARNING: failed to clean up orphaned ImageStream %s: %v", req.Name, err)
			}
		}
	}()

	// Handle internal registry: create SA token secret, ensure ImageStream, generate output ref
	if req.UseInternalRegistry {
		restCfg, err := getRESTConfigFromRequestFn(c)
		if err != nil {
			c.JSON(http.StatusInternalServerError, gin.H{"error": fmt.Sprintf("failed to get REST config: %v", err)})
			return
		}

		secretName, err := createInternalRegistrySecretFn(ctx, restCfg, namespace, req.Name)
		if err != nil {
			c.JSON(http.StatusInternalServerError, gin.H{"error": fmt.Sprintf("failed to create internal registry secret: %v", err)})
			return
		}
		pushSecretName = secretName

		created, err := ensureImageStream(ctx, k8sClient, namespace, req.Name)
		if err != nil {
			c.JSON(http.StatusInternalServerError, gin.H{"error": fmt.Sprintf("failed to ensure ImageStream: %v", err)})
			return
		}
		createdImageStream = created

		req.Output = generateRegistryImageRef(defaultInternalRegistryURL, namespace, req.Name, "latest")
	}

	if req.Output == "" {
		c.JSON(http.StatusBadRequest, gin.H{"error": "output image reference is required"})
		return
	}

	// Check for existing
	existing := &automotivev1alpha1.ContainerBuild{}
	if err := k8sClient.Get(ctx, types.NamespacedName{Name: req.Name, Namespace: namespace}, existing); err == nil {
		c.JSON(http.StatusConflict, gin.H{"error": fmt.Sprintf("ContainerBuild %s already exists", req.Name)})
		return
	} else if !k8serrors.IsNotFound(err) {
		c.JSON(http.StatusInternalServerError, gin.H{"error": fmt.Sprintf("error checking existing build: %v", err)})
		return
	}

	// Create push secret if registry credentials provided (skipped when using internal registry)
	if !req.UseInternalRegistry && req.RegistryCredentials != nil && req.RegistryCredentials.Enabled {
		var err error
		pushSecretName, err = createPushSecret(ctx, k8sClient, namespace, req.Name, req.RegistryCredentials)
		if err != nil {
			c.JSON(http.StatusInternalServerError, gin.H{"error": fmt.Sprintf("error creating push secret: %v", err)})
			return
		}
	}

	containerBuild := &automotivev1alpha1.ContainerBuild{
		ObjectMeta: metav1.ObjectMeta{
			Name:      req.Name,
			Namespace: namespace,
			Labels: map[string]string{
				"app.kubernetes.io/managed-by": "build-api",
				"app.kubernetes.io/part-of":    "automotive-dev",
				"app.kubernetes.io/created-by": "automotive-dev-build-api",
			},
			Annotations: map[string]string{
				"automotive.sdv.cloud.redhat.com/requested-by": requestedBy,
			},
		},
		Spec: automotivev1alpha1.ContainerBuildSpec{
			Output:                req.Output,
			Containerfile:         req.Containerfile,
			Strategy:              req.Strategy,
			BuildArgs:             req.BuildArgs,
			Architecture:          req.Architecture,
			Timeout:               req.Timeout,
			PushSecretRef:         pushSecretName,
			UseServiceAccountAuth: req.UseInternalRegistry,
		},
	}

	if err := k8sClient.Create(ctx, containerBuild); err != nil {
		c.JSON(http.StatusInternalServerError, gin.H{"error": fmt.Sprintf("error creating ContainerBuild: %v", err)})
		return
	}
	committed = true

	// Set owner reference on push secret for cascading deletion
	if pushSecretName != "" {
		if err := setContainerBuildSecretOwnerRef(ctx, k8sClient, namespace, pushSecretName, containerBuild); err != nil {
			log.Printf(
				"WARNING: failed to set owner reference on push secret %s: %v "+
					"(cleanup may require manual intervention)",
				pushSecretName, err,
			)
		}
	}

	outputImage := req.Output
	if req.UseInternalRegistry {
		externalRoute, _ := getExternalRegistryRoute(ctx, k8sClient, namespace)
		if externalRoute != "" {
			outputImage = translateToExternalURL(outputImage, externalRoute)
		}
	}

	writeJSON(c, http.StatusAccepted, ContainerBuildResponse{
		Name:        req.Name,
		Phase:       phasePending,
		Message:     "Container build created",
		RequestedBy: requestedBy,
		OutputImage: outputImage,
	})
}

func listContainerBuilds(c *gin.Context) {
	namespace := resolveNamespace()

	k8sClient, err := getClientFromRequest(c)
	if err != nil {
		c.JSON(http.StatusInternalServerError, gin.H{"error": fmt.Sprintf("k8s client error: %v", err)})
		return
	}

	ctx := c.Request.Context()
	cbList := &automotivev1alpha1.ContainerBuildList{}
	if err := k8sClient.List(ctx, cbList, client.InNamespace(namespace)); err != nil {
		c.JSON(http.StatusInternalServerError, gin.H{"error": fmt.Sprintf("error listing container builds: %v", err)})
		return
	}

	// Sort by creation time, newest first
	sort.Slice(cbList.Items, func(i, j int) bool {
		return cbList.Items[j].CreationTimestamp.Before(&cbList.Items[i].CreationTimestamp)
	})

	items := make([]ContainerBuildListItem, 0, len(cbList.Items))
	for _, cb := range cbList.Items {
		item := ContainerBuildListItem{
			Name:        cb.Name,
			Phase:       cb.Status.Phase,
			Message:     cb.Status.Message,
			CreatedAt:   cb.CreationTimestamp.Format(time.RFC3339),
			OutputImage: cb.Spec.Output,
		}
		if v, ok := cb.Annotations["automotive.sdv.cloud.redhat.com/requested-by"]; ok {
			item.RequestedBy = v
		}
		if cb.Status.CompletionTime != nil {
			item.CompletionTime = cb.Status.CompletionTime.Format(time.RFC3339)
		}
		items = append(items, item)
	}

	writeJSON(c, http.StatusOK, items)
}

func (a *APIServer) getContainerBuild(c *gin.Context, name string) {
	namespace := resolveNamespace()

	k8sClient, err := getClientFromRequest(c)
	if err != nil {
		c.JSON(http.StatusInternalServerError, gin.H{"error": fmt.Sprintf("k8s client error: %v", err)})
		return
	}

	ctx := c.Request.Context()
	cb := &automotivev1alpha1.ContainerBuild{}
	if err := k8sClient.Get(ctx, types.NamespacedName{Name: name, Namespace: namespace}, cb); err != nil {
		if k8serrors.IsNotFound(err) {
			c.JSON(http.StatusNotFound, gin.H{"error": "not found"})
			return
		}
		c.JSON(http.StatusInternalServerError, gin.H{"error": fmt.Sprintf("error fetching container build: %v", err)})
		return
	}

	outputImage := cb.Spec.Output
	if cb.Spec.UseServiceAccountAuth {
		externalRoute, _ := getExternalRegistryRoute(ctx, k8sClient, namespace)
		if externalRoute != "" {
			outputImage = translateToExternalURL(outputImage, externalRoute)
		}
	}

	resp := ContainerBuildResponse{
		Name:        cb.Name,
		Phase:       cb.Status.Phase,
		Message:     cb.Status.Message,
		OutputImage: outputImage,
		ImageDigest: cb.Status.ImageDigest,
	}
	if v, ok := cb.Annotations["automotive.sdv.cloud.redhat.com/requested-by"]; ok {
		resp.RequestedBy = v
	}
	if cb.Status.StartTime != nil {
		resp.StartTime = cb.Status.StartTime.Format(time.RFC3339)
	}
	if cb.Status.CompletionTime != nil {
		resp.CompletionTime = cb.Status.CompletionTime.Format(time.RFC3339)
	}

	// Mint a fresh registry token for completed/failed internal registry builds
	if cb.Spec.UseServiceAccountAuth &&
		(cb.Status.Phase == phaseCompleted || cb.Status.Phase == phaseFailed) {
		token, tokenErr := a.mintRegistryToken(ctx, c, namespace)
		if tokenErr != nil {
			a.log.Error(tokenErr, "failed to mint registry token for container build", "build", name)
		} else {
			resp.RegistryToken = token
		}
	}

	writeJSON(c, http.StatusOK, resp)
}

func parseWaiterLockFileArg(tokens []string) (string, bool) {
	for i := 0; i < len(tokens); i++ {
		token := tokens[i]
		if strings.HasPrefix(token, "--lock-file=") {
			lockFile := strings.TrimSpace(strings.TrimPrefix(token, "--lock-file="))
			if lockFile != "" {
				return lockFile, true
			}
			continue
		}
		if token == "--lock-file" && i+1 < len(tokens) {
			lockFile := strings.TrimSpace(tokens[i+1])
			if lockFile != "" {
				return lockFile, true
			}
		}
	}
	return "", false
}

func getWaiterLockFileFromPodSpec(pod *corev1.Pod, containerName string) (string, bool) {
	for _, container := range pod.Spec.Containers {
		if container.Name != containerName {
			continue
		}
		tokens := append(append([]string{}, container.Command...), container.Args...)
		return parseWaiterLockFileArg(tokens)
	}

	for _, container := range pod.Spec.InitContainers {
		if container.Name != containerName {
			continue
		}
		tokens := append(append([]string{}, container.Command...), container.Args...)
		return parseWaiterLockFileArg(tokens)
	}

	return "", false
}

// findWaiterContainer returns the name of the running source-local waiter container in a build pod.
func findWaiterContainer(pod *corev1.Pod) string {
	// Check init containers for backward compatibility
	for _, initCS := range pod.Status.InitContainerStatuses {
		if strings.Contains(initCS.Name, "source-local") && initCS.State.Running != nil {
			return initCS.Name
		}
	}
	// Modern Shipwright uses regular containers, not init containers
	for _, cs := range pod.Status.ContainerStatuses {
		if strings.Contains(cs.Name, "source-local") && cs.State.Running != nil {
			return cs.Name
		}
	}
	// Final fallback: any running init container with a source-related name
	for _, initCS := range pod.Status.InitContainerStatuses {
		if initCS.State.Running != nil && strings.Contains(initCS.Name, "source") {
			return initCS.Name
		}
	}
	return ""
}

func (a *APIServer) uploadContainerBuildContext(c *gin.Context, name string) {
	namespace := resolveNamespace()

	k8sClient, err := getClientFromRequestFn(c)
	if err != nil {
		c.JSON(http.StatusInternalServerError, gin.H{"error": fmt.Sprintf("k8s client error: %v", err)})
		return
	}

	ctx := c.Request.Context()
	cb := &automotivev1alpha1.ContainerBuild{}
	if err := k8sClient.Get(ctx, types.NamespacedName{Name: name, Namespace: namespace}, cb); err != nil {
		if k8serrors.IsNotFound(err) {
			c.JSON(http.StatusNotFound, gin.H{"error": "not found"})
			return
		}
		c.JSON(http.StatusInternalServerError, gin.H{"error": fmt.Sprintf("error fetching container build: %v", err)})
		return
	}

	if cb.Status.Phase != phaseUploading && cb.Status.Phase != phasePending {
		c.JSON(http.StatusConflict, gin.H{
			"error": fmt.Sprintf("cannot upload context in phase %q, must be Uploading or Pending", cb.Status.Phase),
		})
		return
	}

	buildRunName := cb.Status.BuildRunName
	if buildRunName == "" {
		c.JSON(http.StatusServiceUnavailable, gin.H{"error": "BuildRun not yet created, try again shortly"})
		return
	}

	// Find the BuildRun's pod
	podList := &corev1.PodList{}
	if err := k8sClient.List(ctx, podList,
		client.InNamespace(namespace),
		client.MatchingLabels{"buildrun.shipwright.io/name": buildRunName},
	); err != nil {
		c.JSON(http.StatusInternalServerError, gin.H{"error": fmt.Sprintf("error listing pods: %v", err)})
		return
	}

	if len(podList.Items) == 0 {
		c.JSON(http.StatusServiceUnavailable, gin.H{"error": "build pod not yet created, try again shortly"})
		return
	}

	// Find the pod that has a running waiter container
	var buildPod *corev1.Pod
	var waiterContainer string
	for i := range podList.Items {
		if name := findWaiterContainer(&podList.Items[i]); name != "" {
			buildPod = &podList.Items[i]
			waiterContainer = name
			break
		}
	}

	if buildPod == nil {
		c.JSON(http.StatusServiceUnavailable, gin.H{"error": "source waiter container not yet running, try again shortly"})
		return
	}

	// Stream the request body (tarball) into the waiter container
	restCfg, err := getRESTConfigFromRequest(c)
	if err != nil {
		c.JSON(http.StatusInternalServerError, gin.H{"error": fmt.Sprintf("error getting REST config: %v", err)})
		return
	}

	clientset, err := kubernetes.NewForConfig(restCfg)
	if err != nil {
		c.JSON(http.StatusInternalServerError, gin.H{"error": fmt.Sprintf("error creating clientset: %v", err)})
		return
	}

	// Phase 1: Stream the uncompressed tarball into the waiter container.
	tarCmd := []string{"tar", "--no-same-permissions", "--no-same-owner", "-xf", "-", "-C", "/workspace/source"}

	execReq := clientset.CoreV1().RESTClient().Post().
		Resource("pods").
		Name(buildPod.Name).
		Namespace(namespace).
		SubResource("exec").
		VersionedParams(&corev1.PodExecOptions{
			Container: waiterContainer,
			Command:   tarCmd,
			Stdin:     true,
			Stdout:    true,
			Stderr:    true,
			TTY:       false,
		}, kscheme.ParameterCodec)

	executor, err := remotecommand.NewSPDYExecutor(restCfg, http.MethodPost, execReq.URL())
	if err != nil {
		c.JSON(http.StatusInternalServerError, gin.H{"error": fmt.Sprintf("error creating executor: %v", err)})
		return
	}

	var tarStderr strings.Builder
	streamOpts := remotecommand.StreamOptions{
		Stdin:  c.Request.Body,
		Stdout: io.Discard,
		Stderr: &tarStderr,
	}

	if err := executor.StreamWithContext(ctx, streamOpts); err != nil {
		detail := strings.TrimSpace(tarStderr.String())
		if detail != "" {
			c.JSON(http.StatusInternalServerError, gin.H{"error": fmt.Sprintf("error streaming context to pod: %v: %s", err, detail)})
		} else {
			c.JSON(http.StatusInternalServerError, gin.H{"error": fmt.Sprintf("error streaming context to pod: %v", err)})
		}
		return
	}

	// Phase 2: Signal completion to the waiter.
	// Use the same lock file only when it is explicitly configured on the source-local step.
	doneCmd := []string{"waiter", "done"}
	if lockFile, ok := getWaiterLockFileFromPodSpec(buildPod, waiterContainer); ok {
		doneCmd = append(doneCmd, "--lock-file="+lockFile)
	}

	doneExecReq := clientset.CoreV1().RESTClient().Post().
		Resource("pods").
		Name(buildPod.Name).
		Namespace(namespace).
		SubResource("exec").
		VersionedParams(&corev1.PodExecOptions{
			Container: waiterContainer,
			Command:   doneCmd,
			Stdin:     false,
			Stdout:    true,
			Stderr:    true,
			TTY:       false,
		}, kscheme.ParameterCodec)

	doneExecutor, err := remotecommand.NewSPDYExecutor(restCfg, http.MethodPost, doneExecReq.URL())
	if err != nil {
		c.JSON(http.StatusInternalServerError, gin.H{"error": fmt.Sprintf("error creating done executor: %v", err)})
		return
	}

	var doneStdout strings.Builder
	var doneStderr strings.Builder
	doneStreamOpts := remotecommand.StreamOptions{
		Stdout: &doneStdout,
		Stderr: &doneStderr,
	}

	if err := doneExecutor.StreamWithContext(ctx, doneStreamOpts); err != nil {
		detail := strings.TrimSpace(doneStderr.String())
		if detail == "" {
			detail = strings.TrimSpace(doneStdout.String())
		}
		if detail != "" {
			c.JSON(http.StatusInternalServerError, gin.H{"error": fmt.Sprintf("error signaling completion: %v: %s", err, detail)})
			return
		}
		c.JSON(http.StatusInternalServerError, gin.H{"error": fmt.Sprintf("error signaling completion: %v", err)})
		return
	}

	writeJSON(c, http.StatusOK, map[string]string{"status": "ok", "message": "context uploaded successfully"})
}
